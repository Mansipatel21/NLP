import nltk
from nltk.tokenize import word_tokenize, sent_tokenize

# Sample text
text = "Tokenization is the process of dividing text into smaller units. NLTK is a popular library for natural language processing."

# Word tokenization
words = word_tokenize(text)
print("Word Tokens:", words)

# Sentence tokenization
sentences = sent_tokenize(text)
print("Sentences:", sentences)
